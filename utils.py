"""
ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã€ç”»é¢è¡¨ç¤ºä»¥å¤–ã®æ§˜ã€…ãªé–¢æ•°å®šç¾©ã®ãƒ•ã‚¡ã‚¤ãƒ«ã§ã™ã€‚
"""

############################################################
# ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®èª­ã¿è¾¼ã¿
############################################################
import os
import time
from dotenv import load_dotenv
import streamlit as st
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.schema import HumanMessage
from langchain_openai import ChatOpenAI
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
import constants as ct


############################################################
# è¨­å®šé–¢é€£
############################################################
# ã€Œ.envã€ãƒ•ã‚¡ã‚¤ãƒ«ã§å®šç¾©ã—ãŸç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
load_dotenv()


############################################################
# é–¢æ•°å®šç¾©
############################################################

def get_source_icon(source):
    """
    ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ä¸€ç·’ã«è¡¨ç¤ºã™ã‚‹ã‚¢ã‚¤ã‚³ãƒ³ã®ç¨®é¡ã‚’å–å¾—

    Args:
        source: å‚ç…§å…ƒã®ã‚ã‚Šã‹

    Returns:
        ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ä¸€ç·’ã«è¡¨ç¤ºã™ã‚‹ã‚¢ã‚¤ã‚³ãƒ³ã®ç¨®é¡
    """
    # å‚ç…§å…ƒãŒWebãƒšãƒ¼ã‚¸ã®å ´åˆã¨ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã§ã€å–å¾—ã™ã‚‹ã‚¢ã‚¤ã‚³ãƒ³ã®ç¨®é¡ã‚’å¤‰ãˆã‚‹
    if source.startswith("http"):
        icon = ct.LINK_SOURCE_ICON
    else:
        icon = ct.DOC_SOURCE_ICON
    
    return icon


def build_error_message(message):
    """
    ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ç®¡ç†è€…å•ã„åˆã‚ã›ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®é€£çµ

    Args:
        message: ç”»é¢ä¸Šã«è¡¨ç¤ºã™ã‚‹ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸

    Returns:
        ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ç®¡ç†è€…å•ã„åˆã‚ã›ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®é€£çµãƒ†ã‚­ã‚¹ãƒˆ
    """
    return "\n".join([message, ct.COMMON_ERROR_MESSAGE])


def get_llm_response(chat_message, progress_callback=None):
    """
    LLMã‹ã‚‰ã®å›ç­”å–å¾—

    Args:
        chat_message: ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›å€¤
        progress_callback: é€²æ—è¡¨ç¤ºç”¨ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•°

    Returns:
        LLMã‹ã‚‰ã®å›ç­”
    """
    start_time = time.time()
    
    # é€²æ—æ›´æ–°ç”¨ã®ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
    def update_progress(message):
        if progress_callback:
            elapsed = time.time() - start_time
            progress_callback(f"{message} ({elapsed:.1f}ç§’çµŒé)")
    
    # ã‚¹ãƒ†ãƒƒãƒ—1: LLMã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®æº–å‚™
    update_progress("ğŸ”§ AI ãƒ¢ãƒ‡ãƒ«ã‚’æº–å‚™ã—ã¦ã„ã¾ã™...")
    llm = ChatOpenAI(model_name=ct.MODEL, temperature=ct.TEMPERATURE)
    time.sleep(0.5)  # è¦–è¦šçš„ãªé€²æ—ç¢ºèªã®ãŸã‚ã®çŸ­ã„å¾…æ©Ÿ

    # ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®ä½œæˆ
    update_progress("ğŸ“ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ã„ã¾ã™...")
    
    # ä¼šè©±å±¥æ­´ãªã—ã§ã‚‚LLMã«ç†è§£ã—ã¦ã‚‚ã‚‰ãˆã‚‹ã€ç‹¬ç«‹ã—ãŸå…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ã™ã‚‹ãŸã‚ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½œæˆ
    question_generator_template = ct.SYSTEM_PROMPT_CREATE_INDEPENDENT_TEXT
    question_generator_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", question_generator_template),
            MessagesPlaceholder("chat_history"),
            ("human", "{input}")
        ]
    )

    # ãƒ¢ãƒ¼ãƒ‰ã«ã‚ˆã£ã¦LLMã‹ã‚‰å›ç­”ã‚’å–å¾—ã™ã‚‹ç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å¤‰æ›´
    if st.session_state.mode == ct.ANSWER_MODE_1:
        # ãƒ¢ãƒ¼ãƒ‰ãŒã€Œç¤¾å†…æ–‡æ›¸æ¤œç´¢ã€ã®å ´åˆã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        question_answer_template = ct.SYSTEM_PROMPT_DOC_SEARCH
    else:
        # ãƒ¢ãƒ¼ãƒ‰ãŒã€Œç¤¾å†…å•ã„åˆã‚ã›ã€ã®å ´åˆã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        question_answer_template = ct.SYSTEM_PROMPT_INQUIRY
    # LLMã‹ã‚‰å›ç­”ã‚’å–å¾—ã™ã‚‹ç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½œæˆ
    question_answer_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", question_answer_template),
            MessagesPlaceholder("chat_history"),
            ("human", "{input}")
        ]
    )
    time.sleep(0.5)  # è¦–è¦šçš„ãªé€²æ—ç¢ºèªã®ãŸã‚ã®çŸ­ã„å¾…æ©Ÿ

    # ã‚¹ãƒ†ãƒƒãƒ—3: é–¢é€£æ–‡æ›¸ã®æ¤œç´¢
    update_progress("ğŸ” é–¢é€£æ–‡æ›¸ã‚’æ¤œç´¢ã—ã¦ã„ã¾ã™...")
    
    # ä¼šè©±å±¥æ­´ãªã—ã§ã‚‚LLMã«ç†è§£ã—ã¦ã‚‚ã‚‰ãˆã‚‹ã€ç‹¬ç«‹ã—ãŸå…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ã™ã‚‹ãŸã‚ã®Retrieverã‚’ä½œæˆ
    history_aware_retriever = create_history_aware_retriever(
        llm, st.session_state.retriever, question_generator_prompt
    )

    # ã‚¹ãƒ†ãƒƒãƒ—4: å›ç­”ç”Ÿæˆãƒã‚§ãƒ¼ãƒ³ã®ä½œæˆ
    update_progress("âš™ï¸ å›ç­”ç”Ÿæˆãƒã‚§ãƒ¼ãƒ³ã‚’æ§‹ç¯‰ã—ã¦ã„ã¾ã™...")
    
    # LLMã‹ã‚‰å›ç­”ã‚’å–å¾—ã™ã‚‹ç”¨ã®Chainã‚’ä½œæˆ
    question_answer_chain = create_stuff_documents_chain(llm, question_answer_prompt)
    # ã€ŒRAG x ä¼šè©±å±¥æ­´ã®è¨˜æ†¶æ©Ÿèƒ½ã€ã‚’å®Ÿç¾ã™ã‚‹ãŸã‚ã®Chainã‚’ä½œæˆ
    chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)
    time.sleep(0.5)  # è¦–è¦šçš„ãªé€²æ—ç¢ºèªã®ãŸã‚ã®çŸ­ã„å¾…æ©Ÿ

    # ã‚¹ãƒ†ãƒƒãƒ—5: AIå›ç­”ã®ç”Ÿæˆ
    update_progress("ğŸ¤– AI ãŒå›ç­”ã‚’ç”Ÿæˆã—ã¦ã„ã¾ã™...")
    
    # LLMã¸ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã¨ãƒ¬ã‚¹ãƒãƒ³ã‚¹å–å¾—
    llm_response = chain.invoke({"input": chat_message, "chat_history": st.session_state.chat_history})
    
    # ã‚¹ãƒ†ãƒƒãƒ—6: ä¼šè©±å±¥æ­´ã®æ›´æ–°
    update_progress("ğŸ’¾ ä¼šè©±å±¥æ­´ã‚’æ›´æ–°ã—ã¦ã„ã¾ã™...")
    
    # LLMãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ä¼šè©±å±¥æ­´ã«è¿½åŠ 
    st.session_state.chat_history.extend([HumanMessage(content=chat_message), llm_response["answer"]])
    
    # å®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    total_time = time.time() - start_time
    update_progress(f"âœ… å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼ (åˆè¨ˆ: {total_time:.1f}ç§’)")
    time.sleep(1)  # å®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤ºã™ã‚‹ãŸã‚ã®å¾…æ©Ÿ

    return llm_response